---
title: "Planned Analysis Code"
author: "Matteo Lisi, Erin Buchanan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r}
library(dplyr)
library(lme4)
library(kableExtra)
library(tidyr)
library(stringr)
```

## Data

```{r}
d <- read.csv("../manipulation-check/manip_test_noOverlap_anonim.csv")
```

## Exclusions

The following table provides a breakdown of the number of participants excluded based on each criterion.

```{r}
total_initial <- nrow(d)

# Status == 0
excluded_status <- total_initial - nrow(d %>% filter(Status == 0))
remaining_after_status <- nrow(d %>% filter(Status == 0))


# Finished == 1
excluded_finished <- remaining_after_status - nrow(d %>% filter(Status == 0, 
                                                                Finished == 1))
#excluded_finished <- total_initial - nrow(d %>% filter(Finished == 1))
remaining_after_finished <- nrow(d %>% filter(Status == 0, 
                                              Finished == 1))

# Q_RecaptchaScore > 0.5
excluded_recaptcha <- remaining_after_finished - nrow(d %>% filter(Status == 0, 
                                                                   Finished == 1, 
                                                                   Progress >= 99, 
                                                                   Q_RecaptchaScore >= 0.5))
remaining_after_recaptcha <- nrow(d %>% filter(Status == 0, 
                                               Finished == 1, 
                                               Progress >= 99, 
                                               Q_RecaptchaScore >= 0.5))

# Total number of participants excluded (excluded because they fail at least 1 criterion)
total_excluded <- total_initial - remaining_after_recaptcha

# Create summary table
summary_table <- data.frame(
  Criterion = c( "Status == 0",
                 "Finished == 1", 
                 "Q_RecaptchaScore >= 0.5", 
                 "Total"),
  Excluded = c(excluded_status, 
               excluded_finished, 
               excluded_recaptcha, 
               total_excluded),
  Fraction = c(excluded_status/total_initial, 
               excluded_finished/total_initial, 
               excluded_recaptcha/total_initial, 
               total_excluded/total_initial),
  
  Percentage = c(excluded_status/total_initial * 100, 
               excluded_finished/total_initial * 100, 
               excluded_recaptcha/total_initial * 100, 
               total_excluded/total_initial * 100)
)

# Print the summary table as an HTML table
kable(summary_table, format = "html", booktabs = TRUE, 
      col.names = c("Exclusion Criterion", "Number Excluded", "Fraction Excluded", "Percentage Excluded"), 
      caption="Each line indicate how many participants failed each criterion. Exclusion criteria are applied sequentially in order; the last line indicate how many participants failed at least one criterion in total.") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Remove participants who failed at least one of the above criteria.

```{r}
d <- d %>% 
  filter(Consent == 1, 
         Progress >= 99, 
         Finished == 1, 
         Status == 0, 
         Q_RecaptchaScore >= 0.5)
```

Check for duplicated responses (same Prolific ID with more than one response):

```{r}
n_pid <- tapply(d$PROLIFIC_PID, d$PROLIFIC_PID, length)
length(n_pid[n_pid==2])
```

These participants completed the survey twice:

```{r}
duplicate_id <- names(n_pid[n_pid==2])
duplicate_id
```

To ensure data quality, only the first response (based on response date and time) for each duplicate ID will be included in the analysis:

```{r}
# keep in only the first response for each duplicate
duplicates <- d %>%
  filter(PROLIFIC_PID %in% duplicate_id) %>%
  arrange(PROLIFIC_PID, StartDate, EndDate) %>%
  group_by(PROLIFIC_PID) %>%
  dplyr::slice(1) %>% # slice operate on each group for grouped df
  ungroup()

# non-duplicate responses
non_duplicates <- d %>%
  filter(!PROLIFIC_PID %in% duplicate_id)

# put them back together
d <- bind_rows(duplicates, non_duplicates)
```

## Demographics information of participants

```{r}
demographic_summary <- d %>%
  summarise(
    `Avearge age (Std.)` = sprintf("%.2f (%.2f)", mean(Age, na.rm = TRUE), sd(Age, na.rm = TRUE)),
    `Age Range` = paste(min(Age, na.rm = TRUE), "to", max(Age, na.rm = TRUE)),
    `N. males` = sprintf("%i", sum(Sex == 0, na.rm = TRUE)),
    `N. female` = sprintf("%i", sum(Sex == 1, na.rm = TRUE))
  ) %>%
  pivot_longer(everything(), names_to = "Demographic", values_to = "Value") 

knitr::kable(demographic_summary, format = "html", booktabs = TRUE, caption = "Demographic Information of Participants", col.names = c("Demographic", "Value"))%>%
  kable_styling(latex_options = c("scale_down","striped"))
```

## Data Transformation

To prepare the data for analysis, we transform it into a long format. This is done separately for each condition (CG, BJ, and IJ) and then combined.

```{r}
# CG
crt_CG <- d %>% select(PROLIFIC_PID, matches("^crt[0-9]+_CG"))
crt_CG <- crt_CG %>%
  pivot_longer(
    cols = -PROLIFIC_PID,
    names_to = c("crt", ".value"),
    names_pattern = "(crt\\d+)_CG_(.*)") 
crt_CG$group <- "CG"
crt_CG$explain <- NA

# BJ
crt_BJ <- d %>% select(PROLIFIC_PID, matches("^crt[0-9]+_BJ"))
crt_BJ <- crt_BJ %>%
  pivot_longer(
    cols = -PROLIFIC_PID,
    names_to = c("crt", ".value"),
    names_pattern = "(crt\\d+)_BJ_(.*)") 
crt_BJ$group <- "BJ"

# IJ
crt_IJ <- d %>% select(PROLIFIC_PID, matches("^crt[0-9]+_IJ"))
crt_IJ <- crt_IJ %>%
  pivot_longer(
    cols = -PROLIFIC_PID,
    names_to = c("crt", ".value"),
    names_pattern = "(crt\\d+)_IJ_(.*)") 
crt_IJ$group <- "IJ"

# put together
other_info <- d %>% select(PROLIFIC_PID, !matches("^crt[0-9]")) %>%
  select(PROLIFIC_PID, !starts_with("practice"))
crt_all <- rbind(crt_CG, crt_BJ, crt_IJ)
dat <- left_join(crt_all, other_info, by = "PROLIFIC_PID") %>%
  filter(!is.na(task))

# recode response accuracy as binary 0/1
dat$accuracy <- ifelse(!is.na(dat$task), 
                       ifelse(dat$task==1,1,0),
                       dat$task)

# contrast coding for condition label
dat$group <- factor(dat$group)
contrasts(dat$group) <- contr.treatment(levels(dat$group), base=2)

# make a smaller dataset for analysis, with counts of 
# correct and errors for each participant
dag <- dat %>%
  group_by(PROLIFIC_PID, group) %>%
  summarise(correct = sum(accuracy, na.rm=T),
            N = n(),
            accuracy=mean(accuracy, na.rm=T)) %>%
  mutate(errors = N-correct) %>%
  filter(!is.nan(accuracy))
  
# sanity check 
all(dag$N==6)

# display 5 random IDs
dag[is.element(dag$PROLIFIC_PID,sample(dag$PROLIFIC_PID,5)),]
```

## Summary statistics

```{r}
stat_summary <- dat %>%
  group_by(PROLIFIC_PID, group) %>%
  summarise(accuracy = mean(accuracy, na.rm = TRUE),
            # may have trouble with this line if it imports with . or space
            RT = mean(`task_rt_Page.Submit`, na.rm = TRUE)) %>%
  group_by(group)%>%
  summarise(
    Accuracy = sprintf("%.2f (%.2f)", mean(accuracy, na.rm = TRUE), sd(accuracy, na.rm = TRUE)),
    `Response time` = sprintf("%.2f (%.2f)", mean(RT, na.rm = TRUE), sd(RT, na.rm = TRUE))
  )  

knitr::kable(stat_summary, format = "html", booktabs = TRUE, caption = "Summary statistics (accuracy and response time); values reported as mean (Std.) across participants.") %>%
  kable_styling(latex_options = c("scale_down","striped"))
```

## Summary statistics by CRT problem

```{r}
stat_summary <- dat %>%
  group_by(crt, group) %>%
  summarise(
    Accuracy = sprintf("%.2f (%.2f)", mean(accuracy, na.rm = TRUE), sd(accuracy, na.rm = TRUE)),
    RT = sprintf("%.2f (%.2f)", mean(`task_rt_Page.Submit`, na.rm = TRUE), sd(`task_rt_Page.Submit`, na.rm = TRUE))
  )  %>%
  pivot_wider(
    names_from = group, 
    values_from = c(Accuracy, RT),
    names_sep = " "
  )

knitr::kable(stat_summary, format = "html", booktabs = TRUE, caption = "Summary statistics of accuracy and response time for individual CRT problems")%>%
  kable_styling(latex_options = c("scale_down","striped"))
```

The above is an example structure and reorganization of the dataset, along with exclusions. Given this pilot was simply to test the study manipulations, the data does not include country or the second round of correction necessary to demonstrate hypothesis code. Therefore, we generate example data:

```{r}
# these are plausible parameter values based on Sirota 2023 and similar 
# article in the literature

# p(error @ stage 1)
err_mu <- 1.22
err_sd <- 0.92

# p(correct @ stage 2 | error @ stage 1) 
corr_mu <- -2.53 
corr_sd <- 1.12

# set effect sizes ----

# Set effect values in standardized as Cohen's d and rescale 
# to an approximately similar value for the logistic distribution. 
# Specifically, we multiply the d values by the ratio of the standard deviation
# of the logistic distribution to that of the standard normal distribution, 
# which is (pi/sqrt(3)).
d_fdbk <- 0.19 * (pi/sqrt(3)) # feedback minus baseline
d_just <- 0.19 * (pi/sqrt(3)) # justification minus baseline
d_both <- (d_fdbk + d_just) # (feedback + justification) minus baseline 
# Note: we expect the effects of feedback to be additive

# vector of fixed-effects coefficients
beta_lo_p <- c(corr_mu, NA, NA, NA)
beta_lo_p[2] <- d_fdbk
beta_lo_p[3] <- d_just
beta_lo_p[4] <- d_both

# determine the country-level random effect SD
# half the group-level effect size, so that ~95% of countries have 
# population-level effects of the same sign
rfx_sd_country <- (0.19 * (pi/sqrt(3))) / 2 

# power simulation parameters ----
n_trial <- 6

generate_labels <- function(N_c, wratio = 0.5) {
 n_weird <- round(wratio * N_c)
 n_nonweird <- N_c - n_weird
 labels <- c(
 rep("weird", n_weird), 
 rep("non-weird", n_nonweird)
 )[sample(N_c)]
 return(labels)
}

gen_data <- function(N, N_c, wratio){
 
 N_tot <- N * N_c * 4 # 4 is the number of between subjects conditions
 
 ## simulate dataset
 # simulate error propensity of individual participants in phase 1
 # simulate number of errors in phase 1
 # number of errors (log(errors))
 error_lo <- rnorm(N_tot, mean = err_mu, sd = err_sd)
 
 # simulate random effects and between-individual variability 
 # (to introduce heterogeneity and make simulations more realistic)
 rfx_id <- rnorm(N_tot, mean = 0, sd = corr_sd)
 
 # sample countries
 rfx_country <- rnorm(N_c, mean = 0, sd = rfx_sd_country)
 group_country <- generate_labels(N_c, wratio) 
 
 # create dataset frame
 dat <- expand_grid(country = 1:N_c, 
   condition = 1:4, 
   id = 1:N, 
   trial = 1:6) %>%
 mutate(id = str_c(id, country, condition, sep = "_"))
 dat$country_group <- group_country[dat$country]
 
 # simulate responses
 dat$corrected <- NA
 dat$error <- NA
 dat$id <- as.integer(factor(dat$id, labels = 1:length(unique(dat$id))))
 dat$lo <- NA
 dat$error_lo <- error_lo[dat$id]
 dat$rfx_country <- rfx_country[dat$country]
 dat$rfx_id <- rfx_id[dat$id]
 
 # set log-odds of p(correction) for each datapoint
 dat <- dat %>%
 # control group, baseline who don't get anything (beta_lo_p[1])
 # intercept (control group mean because X1 is the group they are in)
 # intercept beta_lo_p[1] + group*0 + country random + person random effect
 mutate(lo = beta_lo_p[1] + rfx_country + rfx_id) %>%
 mutate(lo = case_when(
 # when group 1 baseline keep the original estimation from the line above 
 condition == 1 ~ lo, 
 # when group is not baseline (feedback, justification, both) then add in the effect of those other conditions 
 condition == 2 ~ lo + beta_lo_p[2], 
 condition == 3 ~ lo + beta_lo_p[3], 
 condition == 4 ~ lo + beta_lo_p[4]
 ))
 
 # simulate number of errors likely for participants
 dat$error <- rbinom(nrow(dat), 
   size = 1, 
   p = exp(dat$error_lo)/(1 + exp(dat$error_lo)))
 
 # simulate number of errors that are corrected in phase 2
 dat$corrected <- ifelse(dat$error == 1, 
    rbinom(nrow(dat), 
     size = 1, 
     p = exp(dat$lo)/(1 + exp(dat$lo))), 
    0)
 
 # total correct responses given the simulated data
 dat$correct <- 1-dat$error + dat$corrected
 
 # keep only trials with errors in phase 1
 dat <- dat %>%
 # filter(error == 1) %>%
 mutate(c_F = ifelse(condition == 2, 1, 0), 
  c_FR = ifelse(condition == 3, 1, 0)) %>%
 mutate(feedback = ifelse(condition == 2 | condition == 4, 1, 0), 
  justification = ifelse(condition == 3 | condition == 4, 1, 0), 
  condition = case_when(
  condition == 1 ~ "baseline", 
  condition == 2 ~ "feedback", 
  condition == 3 ~ "justification", 
  condition == 4 ~ "feedback + justification"
  ))
 
 dat$country_group <- factor(dat$country_group)
 
 # Set attributes
 attr(dat, "N") <- N
 attr(dat, "N_c") <- N_c
 
 # return simulated data
 return(dat)
}

dat <- gen_data(N = (13000/30), N_c = 30, wratio = .3)

dat_H1 <- dat %>%
 filter(correct == 1)

dat_Hother <- dat %>%
 filter(error == 1)
```

```{r}
## TEST HYPOTHESES

alpha <- 0.05
# note: the tests on separate country_groups are independent one another 
# (different countries, participants, labs, etc.), so the expected 
# false positive rate of the composite test, run on the 2 country_group 
# separately and using the conventional alpha = 0.05 for each test, 
# would be 0.05^2 = 0.0025 (the hypothesis supported if and only if 
# all 2 independent tests are significant). We don't also apply correction 
# for multiple comparisons otherwise it would become too conservative.
```

## Question 1A

Do people tend to correct their intuitive errors during deliberation?

### Hypothesis 1A 

First, we hypothesize participants in the control condition who provide correct answers in the deliberative phase will primarily do so already in the initial intuitive phase; specifically, we predict that less than half of correct answers will arise from error correction during deliberation.

(The dependent variable is the number of correct responses during deliberation (phase 2), relative to the total number of correctly answered items in both phases.)

In set notation, this would be |C2\C1||C1 ∪ C2| , where C1 and C2 are the items correctly answered in phase 1 and 2, respectively.

### Power

We plan to recruit at least 13,000 participants in total. This is based on simulations informed by previous literature, assuming small effect sizes (Cohen’s d ≥ 0.19) for our manipulations, and taking into account up to 30% attrition rate and imbalance in the number of participants per category up to a 70/30 split. This sample size should provide at least 95% power to test all hypotheses.

### Analysis

We will use a logistic multilevel model to test if the proportion of correct responses made in phase 2 out of all correct responses given in phase 1 and 2 is less than 50% in the baseline condition, using a one-tailed Wald test on the intercept (which corresponds to expected proportion of correct responses in log-odds). This model will be fit on data from all cultural categories taken together.

### Interpretation

Intercept < 0: Hypothesis supported; lower than 50% correct rate.

Intercept ≥ 0: Hypothesis not supported; higher than 50% correct rate.

```{r}
# now fit models for H1 specifically: note that here the dependent
# variable is different as the hypothesis is about the proportion of
# correct response (out of all correct responses) that are corrected
# in phase 2 ('data' arguments is set to 'dat_H1' instead of just 'dat')
#
mH1_all <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_H1,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

# H1
# # p_correction < 0.5

# One-tailed test for H1a
# Testing if the intercept is significantly less than 0
b_H1a <- fixef(mH1_all)["(Intercept)"]
SE_H1a <- sqrt(diag(vcov(mH1_all))["(Intercept)"])
z_H1a <- b_H1a / SE_H1a
p_value_H1a <- pnorm(z_H1a) # left-tailed test
H1a <- ifelse(p_value_H1a < alpha, 1, 0)

H1a
```

## Question 1B

Do people from individualistic and collectivistic cultures tend to correct less than half of their intuitive errors during deliberation or not?

### Hypothesis 1B 

We expect that people will provide less than half of the correct answers during deliberation in both more individualistic and more collectivistic geopolitical regions.

(The dependent variable thus would be the same as for Hypothesis 1A.)

### Power

Power is the same as above.

### Analysis

We will repeat the same analysis as for H1A separately for each cultural category.

### Interpretation 

Intercept < 0 in both cultural contexts: Hypothesis supported; lower than 50% correct rate common across cultures.

Intercept ≥ 0 in one or both cultural contexts: Hypothesis not supported; higher than 50% correct rate at least in one cultural context.

```{r}
# fit country_group-level models for weird and nonweird
mH1_w <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_H1[dat_H1$country_group == "weird", ],
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

mH1_nw <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_H1[dat_H1$country_group == "non-weird", ],
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

# One-tailed tests for H1b
models <- list(mH1_w, mH1_nw)
names <- c("weird", "non-weird")
H1b_pvals <- numeric(length(models))
H1b_results <- numeric(length(models))

for (i in 1:length(models)) {
        b <- fixef(models[[i]])["(Intercept)"]
        SE <- sqrt(diag(vcov(models[[i]]))["(Intercept)"])
        z <- b / SE
        p_value <- pnorm(z) # left-tailed test
        H1b_pvals[i] <- p_value
        H1b_results[i] <- ifelse(p_value < alpha, 1, 0)
}

names(H1b_pvals) <- names
names(H1b_results) <- names

# complete test
H1b <- ifelse(all(H1b_results == 1), 1, 0)

H1b
```
 
## Question 2A

How does feedback on initial answers, as opposed to no feedback, affect an individual’s tendency to correct intuitive errors during deliberation?

### Hypothesis 2A 

We hypothesise that participants provided with accuracy feedback will be more likely to correct their intuitive incorrect answers than those who receive no feedback. 

(The dependent variable is the probability that participants correct their errors in phase 2 for all the other hypotheses. In set notation, this would be E1 ∩ C2 E1, where E1 indicates the items incorrectly answered in phase 1, while C2 indicates the items correctly answered in phase 2.)

### Power

Power is the same as above.

### Analysis

We will use a logistic multilevel model to compare the proportion of intuitive errors corrected in phase 2 between feedback and baseline conditions. The coefficient of feedback will be assessed using a one-tailed Wald test with p < .05 as the criterion.

### Interpretation

Positive feedback coefficient and p < .05: Supports the hypothesis; feedback increases error correction.

Negative coefficient and/or p ≥ .05: Hypothesis not supported; feedback does not increase error correction.

```{r}
# fit overall model
m_all <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_Hother,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

# H2

# feedback increases correction rate

# One-tailed test for H2a
# Testing if the coefficient for 'feedback' is significantly greater than 0
b_H2a <- fixef(m_all)["feedback"]
SE_H2a <- sqrt(diag(vcov(m_all))["feedback"])
z_H2a <- b_H2a / SE_H2a
p_value_H2a <- 1 - pnorm(z_H2a) # right-tailed test
H2a <- ifelse(p_value_H2a < alpha, 1, 0)

H2a
```

## Question 2B

Do people from individualistic and collectivistic cultures tend to correct their intuitive errors more if they receive answer feedback?

### Hypothesis 2B 

We expect that the feedback effect will occur in both more individualistic and more collectivistic geopolitical regions.

(The dependent variable is the same as for hypothesis 2A.)

### Power 

Power is the same as above.

### Analysis 

We will repeat the same analysis as for H2A to evaluate the impact of feedback on error correction separately for each cultural category.

### Interpretation 

Positive feedback coefficient and p < .05 effect in both cultural contexts: Hypothesis supported; feedback increases error correction in both geopolitical region categories.

Feedback effect negative and/or p ≥ .05 in one or both cultural contexts: Hypothesis not supported; feedback does not increase error correction in both geopolitical region categories.
 
```{r} 
# fit country_group-level models for weird and non-weird
m_w <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_Hother[dat_Hother$country_group == "weird", ],
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

m_nw <- glmer(
        corrected ~ feedback * justification +
                (feedback * justification || country),
        family = binomial("logit"),
        data = dat_Hother[dat_Hother$country_group == "non-weird", ],
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5))
)

# One-tailed tests for H2b
models <- list(m_w, m_nw)
names <- c("weird", "non-weird")
H2b_pvals <- numeric(length(models))
H2b_results <- numeric(length(models))

for (i in 1:length(models)) {
        b <- fixef(models[[i]])["feedback"]
        SE <- sqrt(diag(vcov(models[[i]]))["feedback"])
        z <- b / SE
        p_value <- 1 - pnorm(z) # right-tailed test
        H2b_pvals[i] <- p_value
        H2b_results[i] <- ifelse(p_value < alpha, 1, 0)
}

names(H2b_pvals) <- names
names(H2b_results) <- names

# complete test
H2b <- ifelse(all(H2b_results == 1), 1, 0)

H2b
```

## Question 3A

How does requesting justification of answers, as opposed to no justification, affect an individual’s tendency to correct intuitive errors during deliberation?

### Hypothesis 3A 

We hypothesise that participants who know they will be asked to justify their answers will be more likely to correct their intuitive incorrect responses than those in the no justification condition. 

(The dependent variable is the same as for hypothesis 2A.)

### Power 

Power is the same as above.

### Analysis 

We will use a logistic multilevel model to compare the proportion of intuitive errors corrected in phase 2 between the answer-justification and baseline conditions. The coefficient of justification will be assessed using a one-tailed Wald test with p < .05 as the criterion.

### Interpretation 

Positive answer-justification coefficient and p < .05: Supports the hypothesis; requesting justification of answers increases error correction.

Negative answer-justification coefficient and/or p ≥ .05: Hypothesis not supported; requesting justification of answers does not increase error correction.

```{r}
# uses same model all from H2
# H3
# justification increases correction rate

# One-tailed test for H3a
# Testing if the coefficient for 'justification' is significantly greater than 0
b_H3a <- fixef(m_all)["justification"]
SE_H3a <- sqrt(diag(vcov(m_all))["justification"])
z_H3a <- b_H3a / SE_H3a
p_value_H3a <- 1 - pnorm(z_H3a) # right-tailed test
H3a <- ifelse(p_value_H3a < alpha, 1, 0)
```

## Question 3B

Do people from individualistic and collectivistic cultures tend to correct their intuitive errors more if they are asked to justify their answers?

### Hypothesis 3B

We expect that the justification effect will occur in both more individualistic and more collectivistic geopolitical regions.

(The dependent variable is the same as for hypothesis 2A.)

### Power

Power is the same as above.

### Analysis 

We will repeat the same analysis as for H3A to evaluate the impact of request for justification on error correction separately for each cultural category.

### Interpretation 

Positive answer justification coefficient and p < .05 effect in both cultural contexts: Hypothesis supported; requesting justification of answers increases error correction in both geopolitical region categories.

Justification effect negative and/or p ≥ .05 in one or both cultural contexts: Hypothesis not supported; requesting justification of answers does not increase error correction in both geopolitical region categories.

```{r}
# uses same model all from H2

# One-tailed tests for H3b 
models <- list(m_w, m_nw)
names <- c("weird", "non-weird")
H3b_pvals <- numeric(length(models))
H3b_results <- numeric(length(models))

for (i in 1:length(models)) {
b <- fixef(models[[i]])["justification"]
SE <- sqrt(diag(vcov(models[[i]]))["justification"])
z <- b / SE
p_value <- 1 - pnorm(z) # right-tailed test
H3b_pvals[i] <- p_value
H3b_results[i] <- ifelse(p_value < alpha, 1, 0)
}

names(H3b_pvals) <- names
names(H3b_results) <- names

# complete test
H3b <- ifelse(all(H3b_results == 1), 1, 0)

H3b
```